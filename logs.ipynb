{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "m5S_SbG3rFGl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dGWNyQjfmTOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8398ad37-c833-43e4-c8ba-a3c880acb064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  A0187444W.zip\n",
            "   creating: ViT-fine-tuning-with-ColossalAI/\n",
            "  inflating: __MACOSX/._ViT-fine-tuning-with-ColossalAI  \n",
            "  inflating: ViT-fine-tuning-with-ColossalAI/requirements.txt  \n",
            "  inflating: __MACOSX/ViT-fine-tuning-with-ColossalAI/._requirements.txt  \n",
            "  inflating: ViT-fine-tuning-with-ColossalAI/README.md  \n",
            "  inflating: __MACOSX/ViT-fine-tuning-with-ColossalAI/._README.md  \n",
            "  inflating: ViT-fine-tuning-with-ColossalAI/logs.ipynb  \n",
            "  inflating: __MACOSX/ViT-fine-tuning-with-ColossalAI/._logs.ipynb  \n",
            "  inflating: ViT-fine-tuning-with-ColossalAI/run_finetuning.sh  \n",
            "  inflating: __MACOSX/ViT-fine-tuning-with-ColossalAI/._run_finetuning.sh  \n",
            "  inflating: ViT-fine-tuning-with-ColossalAI/args.py  \n",
            "  inflating: __MACOSX/ViT-fine-tuning-with-ColossalAI/._args.py  \n",
            "  inflating: ViT-fine-tuning-with-ColossalAI/vit_finetuning.py  \n",
            "  inflating: __MACOSX/ViT-fine-tuning-with-ColossalAI/._vit_finetuning.py  \n",
            "  inflating: ViT-fine-tuning-with-ColossalAI/data.py  \n",
            "  inflating: __MACOSX/ViT-fine-tuning-with-ColossalAI/._data.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip A0187444W.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('ViT-fine-tuning-with-ColossalAI')"
      ],
      "metadata": {
        "id": "0JiJO7LHAkQv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YstIzbFHrcHW",
        "outputId": "17530669-a691-46e9-9c22-5e248328fa2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.py  data.py  logs.ipynb  README.md  requirements.txt  run_finetuning.sh  vit_finetuning.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run_finetuning.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utN1ljSLreo3",
        "outputId": "e9fbb870-4cc8-4f55-8552-df5bbe2626e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ pip install -r requirements.txt\n",
            "Requirement already satisfied: colossalai>=0.1.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.3.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.61.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.66.2)\n",
            "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.38.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.7.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (13.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (1.11.1.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.6.4)\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.1.99)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.0.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.13.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.1->-r requirements.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 6)) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 6)) (0.15.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (5.1.1)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.2.14)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.12.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.5.35)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (20.25.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.16.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.16.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.1.2)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (42.0.5)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.3.8)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.18.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.22)\n",
            "+ MODEL=google/vit-base-patch16-224\n",
            "+ OUTPUT_PATH=./output_model\n",
            "+ PLUGIN=gemini\n",
            "+ TP_SIZE=2\n",
            "+ PP_SIZE=2\n",
            "+ GPUNUM=1\n",
            "+ EPOCH=5\n",
            "+ WEIGHT_DECAY=0.05\n",
            "+ WARMUP_RATIO=0.3\n",
            "+ for BS in 8 16 32\n",
            "+ for LR in \"1e-4\" \"2e-4\" \"5e-4\"\n",
            "+ colossalai run --nproc_per_node 1 --master_port 29505 vit_finetuning.py --model_name_or_path google/vit-base-patch16-224 --output_path ./output_model --plugin gemini --batch_size 8 --tp_size 2 --pp_size 2 --num_epoch 5 --learning_rate 1e-4 --weight_decay 0.05 --warmup_ratio 0.3\n",
            "2024-04-08 05:32:19.475445: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 05:32:19.475499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 05:32:19.620061: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 05:32:21.711687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "\n",
            "###################\n",
            "Batch Size: 8\n",
            "Learning Rate: 0.0001\n",
            "###################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/08/24 05:32:24] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "preprocessor_config.json: 100%|██████████| 160/160 [00:00<00:00, 804kB/s]\n",
            "Downloading readme: 100%|██████████| 4.95k/4.95k [00:00<00:00, 19.8MB/s]\n",
            "Downloading data: 100%|██████████| 144M/144M [00:05<00:00, 26.0MB/s]\n",
            "Downloading data: 100%|██████████| 18.5M/18.5M [00:00<00:00, 20.3MB/s]\n",
            "Downloading data: 100%|██████████| 17.7M/17.7M [00:00<00:00, 27.4MB/s]\n",
            "Generating train split: 100%|██████████| 1034/1034 [00:00<00:00, 1184.28 examples/s]\n",
            "Generating validation split: 100%|██████████| 133/133 [00:00<00:00, 1208.10 examples/s]\n",
            "Generating test split: 100%|██████████| 128/128 [00:00<00:00, 3052.87 examples/s]\n",
            "config.json: 100%|██████████| 69.7k/69.7k [00:00<00:00, 2.86MB/s]\n",
            "model.safetensors: 100%|██████████| 346M/346M [00:02<00:00, 131MB/s]\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/08/24 05:32:54] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:178 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:206 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 37.49638605117798 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 234.14264106750488 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/08/24 05:37:28] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:237 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 129/129 [00:44<00:00,  2.90it/s, loss=0.114]\n",
            "Evaluation result for epoch 1:                 average_loss=0.0474,                 accuracy=0.9922.\n",
            "Epoch [2]:  49%|████▉     | 63/129 [00:20<00:23,  2.81it/s, loss=0.168]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [2]: 100%|██████████| 129/129 [00:42<00:00,  3.05it/s, loss=0.0316]\n",
            "Evaluation result for epoch 2:                 average_loss=0.0952,                 accuracy=0.9766.\n",
            "Epoch [3]: 100%|██████████| 129/129 [00:42<00:00,  3.01it/s, loss=0.00351]\n",
            "Evaluation result for epoch 3:                 average_loss=0.0508,                 accuracy=0.9844.\n",
            "Epoch [4]: 100%|██████████| 129/129 [00:42<00:00,  3.02it/s, loss=0.00168]\n",
            "Evaluation result for epoch 4:                 average_loss=0.0535,                 accuracy=0.9844.\n",
            "Epoch [5]: 100%|██████████| 129/129 [00:42<00:00,  3.03it/s, loss=0.00143]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0426,                 accuracy=0.9844.\n",
            "[04/08/24 05:41:08] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:241 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:245 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "+ for LR in \"1e-4\" \"2e-4\" \"5e-4\"\n",
            "+ colossalai run --nproc_per_node 1 --master_port 29505 vit_finetuning.py --model_name_or_path google/vit-base-patch16-224 --output_path ./output_model --plugin gemini --batch_size 8 --tp_size 2 --pp_size 2 --num_epoch 5 --learning_rate 2e-4 --weight_decay 0.05 --warmup_ratio 0.3\n",
            "2024-04-08 05:41:21.793687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 05:41:21.793739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 05:41:21.795154: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 05:41:22.926913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "\n",
            "###################\n",
            "Batch Size: 8\n",
            "Learning Rate: 0.0002\n",
            "###################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/08/24 05:41:25] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/08/24 05:41:42] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:178 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:206 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09157538414001465 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.10561990737915039 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/08/24 05:41:44] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:237 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 129/129 [00:42<00:00,  3.03it/s, loss=0.112]\n",
            "Evaluation result for epoch 1:                 average_loss=0.1842,                 accuracy=0.9219.\n",
            "Epoch [2]:  49%|████▉     | 63/129 [00:19<00:20,  3.24it/s, loss=0.338]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [2]: 100%|██████████| 129/129 [00:41<00:00,  3.12it/s, loss=0.789]\n",
            "Evaluation result for epoch 2:                 average_loss=0.0886,                 accuracy=0.9766.\n",
            "Epoch [3]: 100%|██████████| 129/129 [00:41<00:00,  3.07it/s, loss=0.0035]\n",
            "Evaluation result for epoch 3:                 average_loss=0.1385,                 accuracy=0.9531.\n",
            "Epoch [4]: 100%|██████████| 129/129 [00:41<00:00,  3.12it/s, loss=0.00156]\n",
            "Evaluation result for epoch 4:                 average_loss=0.0930,                 accuracy=0.9688.\n",
            "Epoch [5]: 100%|██████████| 129/129 [00:41<00:00,  3.08it/s, loss=0.000993]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0864,                 accuracy=0.9688.\n",
            "[04/08/24 05:45:17] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:241 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "[04/08/24 05:45:18] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:245 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "+ for LR in \"1e-4\" \"2e-4\" \"5e-4\"\n",
            "+ colossalai run --nproc_per_node 1 --master_port 29505 vit_finetuning.py --model_name_or_path google/vit-base-patch16-224 --output_path ./output_model --plugin gemini --batch_size 8 --tp_size 2 --pp_size 2 --num_epoch 5 --learning_rate 5e-4 --weight_decay 0.05 --warmup_ratio 0.3\n",
            "2024-04-08 05:45:31.609260: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 05:45:31.609303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 05:45:31.610679: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 05:45:32.773840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "\n",
            "###################\n",
            "Batch Size: 8\n",
            "Learning Rate: 0.0005\n",
            "###################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/08/24 05:45:34] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/08/24 05:45:52] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:178 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:206 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09180235862731934 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.11776113510131836 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/08/24 05:45:54] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:237 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 129/129 [00:42<00:00,  3.07it/s, loss=0.0372]\n",
            "Evaluation result for epoch 1:                 average_loss=0.1942,                 accuracy=0.9219.\n",
            "Epoch [2]:  49%|████▉     | 63/129 [00:20<00:20,  3.15it/s, loss=0.533]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [2]: 100%|██████████| 129/129 [00:41<00:00,  3.09it/s, loss=1.09]\n",
            "Evaluation result for epoch 2:                 average_loss=0.2378,                 accuracy=0.9297.\n",
            "Epoch [3]: 100%|██████████| 129/129 [00:41<00:00,  3.12it/s, loss=0.671]\n",
            "Evaluation result for epoch 3:                 average_loss=0.2498,                 accuracy=0.9062.\n",
            "Epoch [4]: 100%|██████████| 129/129 [00:41<00:00,  3.08it/s, loss=0.00659]\n",
            "Evaluation result for epoch 4:                 average_loss=0.1647,                 accuracy=0.9453.\n",
            "Epoch [5]: 100%|██████████| 129/129 [00:41<00:00,  3.12it/s, loss=0.0168]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0850,                 accuracy=0.9688.\n",
            "[04/08/24 05:49:27] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:241 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:245 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "+ for BS in 8 16 32\n",
            "+ for LR in \"1e-4\" \"2e-4\" \"5e-4\"\n",
            "+ colossalai run --nproc_per_node 1 --master_port 29505 vit_finetuning.py --model_name_or_path google/vit-base-patch16-224 --output_path ./output_model --plugin gemini --batch_size 16 --tp_size 2 --pp_size 2 --num_epoch 5 --learning_rate 1e-4 --weight_decay 0.05 --warmup_ratio 0.3\n",
            "2024-04-08 05:49:41.308361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 05:49:41.308409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 05:49:41.310294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 05:49:42.585188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "\n",
            "###################\n",
            "Batch Size: 16\n",
            "Learning Rate: 0.0001\n",
            "###################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/08/24 05:49:44] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/08/24 05:50:00] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:178 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:206 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.10607647895812988 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.12250351905822754 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/08/24 05:50:03] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:237 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 64/64 [00:24<00:00,  2.64it/s, loss=0.0673]\n",
            "Evaluation result for epoch 1:                 average_loss=0.0834,                 accuracy=0.9609.\n",
            "Epoch [2]:  48%|████▊     | 31/64 [00:11<00:11,  2.81it/s, loss=0.0413]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [2]: 100%|██████████| 64/64 [00:23<00:00,  2.71it/s, loss=0.00817]\n",
            "Evaluation result for epoch 2:                 average_loss=0.0237,                 accuracy=1.0000.\n",
            "Epoch [3]: 100%|██████████| 64/64 [00:23<00:00,  2.73it/s, loss=0.00469]\n",
            "Evaluation result for epoch 3:                 average_loss=0.0907,                 accuracy=0.9688.\n",
            "Epoch [4]: 100%|██████████| 64/64 [00:23<00:00,  2.75it/s, loss=0.00183]\n",
            "Evaluation result for epoch 4:                 average_loss=0.0587,                 accuracy=0.9688.\n",
            "Epoch [5]: 100%|██████████| 64/64 [00:23<00:00,  2.74it/s, loss=0.00144]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0493,                 accuracy=0.9844.\n",
            "[04/08/24 05:52:03] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:241 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "[04/08/24 05:52:04] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:245 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "+ for LR in \"1e-4\" \"2e-4\" \"5e-4\"\n",
            "+ colossalai run --nproc_per_node 1 --master_port 29505 vit_finetuning.py --model_name_or_path google/vit-base-patch16-224 --output_path ./output_model --plugin gemini --batch_size 16 --tp_size 2 --pp_size 2 --num_epoch 5 --learning_rate 2e-4 --weight_decay 0.05 --warmup_ratio 0.3\n",
            "2024-04-08 05:52:14.919167: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 05:52:14.919219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 05:52:14.920658: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 05:52:16.052408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "\n",
            "###################\n",
            "Batch Size: 16\n",
            "Learning Rate: 0.0002\n",
            "###################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/08/24 05:52:18] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/08/24 05:52:34] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:178 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:206 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09127664566040039 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.10927700996398926 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/08/24 05:52:36] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:237 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 64/64 [00:24<00:00,  2.66it/s, loss=0.229]\n",
            "Evaluation result for epoch 1:                 average_loss=0.0611,                 accuracy=0.9688.\n",
            "Epoch [2]:  48%|████▊     | 31/64 [00:11<00:11,  2.78it/s, loss=0.0751]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [2]: 100%|██████████| 64/64 [00:23<00:00,  2.71it/s, loss=0.131]\n",
            "Evaluation result for epoch 2:                 average_loss=0.1263,                 accuracy=0.9531.\n",
            "Epoch [3]: 100%|██████████| 64/64 [00:23<00:00,  2.73it/s, loss=0.0053]\n",
            "Evaluation result for epoch 3:                 average_loss=0.0879,                 accuracy=0.9766.\n",
            "Epoch [4]: 100%|██████████| 64/64 [00:23<00:00,  2.74it/s, loss=0.000701]\n",
            "Evaluation result for epoch 4:                 average_loss=0.0414,                 accuracy=0.9766.\n",
            "Epoch [5]: 100%|██████████| 64/64 [00:23<00:00,  2.74it/s, loss=0.000638]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0369,                 accuracy=0.9844.\n",
            "[04/08/24 05:54:37] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:241 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:245 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "+ for LR in \"1e-4\" \"2e-4\" \"5e-4\"\n",
            "+ colossalai run --nproc_per_node 1 --master_port 29505 vit_finetuning.py --model_name_or_path google/vit-base-patch16-224 --output_path ./output_model --plugin gemini --batch_size 16 --tp_size 2 --pp_size 2 --num_epoch 5 --learning_rate 5e-4 --weight_decay 0.05 --warmup_ratio 0.3\n",
            "2024-04-08 05:54:49.676043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 05:54:49.676090: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 05:54:49.677531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 05:54:50.790761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "\n",
            "###################\n",
            "Batch Size: 16\n",
            "Learning Rate: 0.0005\n",
            "###################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/08/24 05:54:52] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/08/24 05:55:09] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:178 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:206 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09299969673156738 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.10885953903198242 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/08/24 05:55:11] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:237 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 64/64 [00:24<00:00,  2.65it/s, loss=0.245]\n",
            "Evaluation result for epoch 1:                 average_loss=0.2192,                 accuracy=0.8984.\n",
            "Epoch [2]:  48%|████▊     | 31/64 [00:11<00:12,  2.74it/s, loss=0.566]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [2]: 100%|██████████| 64/64 [00:23<00:00,  2.70it/s, loss=0.107]\n",
            "Evaluation result for epoch 2:                 average_loss=0.3113,                 accuracy=0.8750.\n",
            "Epoch [3]: 100%|██████████| 64/64 [00:23<00:00,  2.72it/s, loss=0.0567]\n",
            "Evaluation result for epoch 3:                 average_loss=0.0890,                 accuracy=0.9609.\n",
            "Epoch [4]: 100%|██████████| 64/64 [00:23<00:00,  2.74it/s, loss=0.0205]\n",
            "Evaluation result for epoch 4:                 average_loss=0.0361,                 accuracy=0.9844.\n",
            "Epoch [5]: 100%|██████████| 64/64 [00:23<00:00,  2.73it/s, loss=0.00149]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0389,                 accuracy=0.9844.\n",
            "[04/08/24 05:57:12] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:241 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:245 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "+ for BS in 8 16 32\n",
            "+ for LR in \"1e-4\" \"2e-4\" \"5e-4\"\n",
            "+ colossalai run --nproc_per_node 1 --master_port 29505 vit_finetuning.py --model_name_or_path google/vit-base-patch16-224 --output_path ./output_model --plugin gemini --batch_size 32 --tp_size 2 --pp_size 2 --num_epoch 5 --learning_rate 1e-4 --weight_decay 0.05 --warmup_ratio 0.3\n",
            "2024-04-08 05:57:24.555864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 05:57:24.555912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 05:57:24.557330: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 05:57:25.678158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "\n",
            "###################\n",
            "Batch Size: 32\n",
            "Learning Rate: 0.0001\n",
            "###################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/08/24 05:57:27] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/08/24 05:57:44] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:178 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:206 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.14278340339660645 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.1685028076171875 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/08/24 05:57:46] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:237 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 32/32 [00:17<00:00,  1.88it/s, loss=0.126]\n",
            "Evaluation result for epoch 1:                 average_loss=0.1208,                 accuracy=0.9609.\n",
            "Epoch [2]:  47%|████▋     | 15/32 [00:07<00:08,  1.91it/s, loss=0.206]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [2]: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s, loss=0.0105]\n",
            "Evaluation result for epoch 2:                 average_loss=0.0590,                 accuracy=0.9766.\n",
            "Epoch [3]: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s, loss=0.0126]\n",
            "Evaluation result for epoch 3:                 average_loss=0.0126,                 accuracy=1.0000.\n",
            "Epoch [4]: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s, loss=0.00142]\n",
            "Evaluation result for epoch 4:                 average_loss=0.0386,                 accuracy=0.9844.\n",
            "Epoch [5]: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s, loss=0.00118]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0172,                 accuracy=0.9922.\n",
            "[04/08/24 05:59:12] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:241 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "[04/08/24 05:59:13] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:245 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "+ for LR in \"1e-4\" \"2e-4\" \"5e-4\"\n",
            "+ colossalai run --nproc_per_node 1 --master_port 29505 vit_finetuning.py --model_name_or_path google/vit-base-patch16-224 --output_path ./output_model --plugin gemini --batch_size 32 --tp_size 2 --pp_size 2 --num_epoch 5 --learning_rate 2e-4 --weight_decay 0.05 --warmup_ratio 0.3\n",
            "2024-04-08 05:59:24.410090: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 05:59:24.410138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 05:59:24.411513: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 05:59:25.549223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "\n",
            "###################\n",
            "Batch Size: 32\n",
            "Learning Rate: 0.0002\n",
            "###################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/08/24 05:59:27] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/08/24 05:59:43] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:178 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:206 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09281015396118164 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.14030241966247559 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/08/24 05:59:46] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:237 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 32/32 [00:17<00:00,  1.88it/s, loss=0.164]\n",
            "Evaluation result for epoch 1:                 average_loss=0.1513,                 accuracy=0.9531.\n",
            "Epoch [2]:  47%|████▋     | 15/32 [00:07<00:08,  1.92it/s, loss=0.0695]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [2]: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s, loss=0.106]\n",
            "Evaluation result for epoch 2:                 average_loss=0.2592,                 accuracy=0.8984.\n",
            "Epoch [3]: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s, loss=0.0025]\n",
            "Evaluation result for epoch 3:                 average_loss=0.0387,                 accuracy=0.9766.\n",
            "Epoch [4]: 100%|██████████| 32/32 [00:16<00:00,  1.96it/s, loss=0.000903]\n",
            "Evaluation result for epoch 4:                 average_loss=0.0295,                 accuracy=0.9844.\n",
            "Epoch [5]: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s, loss=0.000815]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0263,                 accuracy=0.9844.\n",
            "[04/08/24 06:01:12] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:241 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:245 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "+ for LR in \"1e-4\" \"2e-4\" \"5e-4\"\n",
            "+ colossalai run --nproc_per_node 1 --master_port 29505 vit_finetuning.py --model_name_or_path google/vit-base-patch16-224 --output_path ./output_model --plugin gemini --batch_size 32 --tp_size 2 --pp_size 2 --num_epoch 5 --learning_rate 5e-4 --weight_decay 0.05 --warmup_ratio 0.3\n",
            "2024-04-08 06:01:23.825514: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-08 06:01:23.825557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-08 06:01:23.826959: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-08 06:01:24.950305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "\n",
            "###################\n",
            "Batch Size: 32\n",
            "Learning Rate: 0.0005\n",
            "###################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/08/24 06:01:27] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/08/24 06:01:44] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:178 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:206 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09313488006591797 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.10889887809753418 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/08/24 06:01:47] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:237 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 32/32 [00:17<00:00,  1.87it/s, loss=0.39]\n",
            "Evaluation result for epoch 1:                 average_loss=0.1876,                 accuracy=0.9375.\n",
            "Epoch [2]:  47%|████▋     | 15/32 [00:07<00:09,  1.89it/s, loss=0.239]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [2]: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s, loss=0.225]\n",
            "Evaluation result for epoch 2:                 average_loss=0.2695,                 accuracy=0.9297.\n",
            "Epoch [3]: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s, loss=0.226]\n",
            "Evaluation result for epoch 3:                 average_loss=0.1408,                 accuracy=0.9609.\n",
            "Epoch [4]: 100%|██████████| 32/32 [00:16<00:00,  1.96it/s, loss=0.00556]\n",
            "Evaluation result for epoch 4:                 average_loss=0.0209,                 accuracy=0.9922.\n",
            "Epoch [5]: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s, loss=0.00206]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0219,                 accuracy=0.9922.\n",
            "[04/08/24 06:03:12] INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:241 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "                    INFO     colossalai - colossalai - INFO: /content/vit_finetuning.py:245 main    \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n"
          ]
        }
      ]
    }
  ]
}